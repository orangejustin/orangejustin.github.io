---
layout: post
image: /images/benchmark.png
# preprint: https://drive.google.com/file/d/1LrO3mFMCkfpUpakgHAQwarnvHEw-6Bq3/view?usp=sharing
title:  "Benchmarking Bottom-Up Logical Reasoning for Multimodal Models"
authors: "Tianyue Ou, ibo Kong, Yueqi Song,  <strong>Zecheng Li</strong>"
# info: "Probable Asked Questions (PAQ) for Assessing the Credibility of LLM Outputs"
date: 2024-11-16 00:00:00 +00:00
categories: others
code: https://huggingface.co/datasets/multimodal-reasoning/multimodal-reasoning/tree/main
---
While there has been significant progress in curating high-quality text-only logical reasoning benchmarks, there remains a notable gap in the development of comprehensive multimodal logical reasoning benchmarks. Existing multimodal benchmarks tend to focus heavily on knowledge-based tasks, which are biased towards models pre-trained on extensive knowledge bases, making them less accessible to smaller models with limited pretraining on knowledge bases. Moreover, these benchmarks often concentrate on a single type of logical reasoning, lacking the breadth necessary to fully evaluate a modelâ€™s reasoning capabilities across diverse scenarios. This narrow focus limits their utility in assessing general logical reasoning skills in multimodal contexts, underscoring the need for more balanced, diverse, and inclusive multimodal benchmarks.
