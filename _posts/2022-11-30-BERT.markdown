---
layout: post
image: /images/bert.jpg
report: https://drive.google.com/file/d/1eHzka_B4zGXVGLzSn_b-R3y506i8MVqy/view?usp=sharing
title:  "Pre-trained BERT Text Classification Application with Amazon-Massive-Intent Dataset"
authors: "Wenqian, Keyu, Xiaotong, Yunze, <strong>Zecheng Li</strong>"
info: "Encoder-Decoder Neural Network with pre-trained BERT"
date: 2022-11-30 00:00:00 +00:00
categories: others
code: https://github.com/orangejustin/cse-151b-pa4-relxgen5
---
This project is about using a type of artificial intelligence called BERT to understand what people are trying to say. Specifically, the project focuses on classifying different types of text based on their meaning or intent. For example, if someone types "I want to order a pizza," the BERT model will recognize that the intent is to order food. The model uses pre-existing knowledge to do this, which has been gained through being trained on a large amount of data beforehand.